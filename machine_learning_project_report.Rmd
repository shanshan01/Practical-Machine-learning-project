
---
title: "Practical_machine_learning_project"
author: "Shanshan"
date: "Friday, September 19, 2014"
output: html_document
---
##Problem description and goals

The human activity recognition research has traditionally focused on discriminating between different activities, yet quantifying " how (well)'' an activity was performed has only received little attention so far, even though it potentially provides useful information for a large variety of applications, such as sports training. The approach proposed for the the weight lifting exercises dataset is aiming to investigate the problem mentioned above. 

In the weight lifting exercises, six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Among the five classes, only class A corresponds to the specified execution of the exercise, while all the other four classes correspond to common mistakes.

Our goal in this project is to predict the manner in which the subject did the weight lifting exercises, thus investigating " how (well)'' the activity was performed by the wearer. 

##Data preprocessing

### Defining the response variable
Our goal of the analysis is to predict the class which the manner of the exercises the subjects performed belongs to. And the class variable in the data set contains five values: A, B, C, D and E. So we will define four dummy variables to characterize it.
$$
x_1=
\left\{
\begin{array}{ll}
1 & \mbox{if in class A}\\
0 &  \mbox{otherwise}
\end{array}
\right.
$$
$$
x_2=
\left\{
\begin{array}{ll}
1 & \mbox{if in class B}\\
0 &  \mbox{otherwise}
\end{array}
\right.
$$
$$
x_3=
\left\{
\begin{array}{ll}
1 & \mbox{if in class C}\\
0 &  \mbox{otherwise}
\end{array}
\right.
$$
$$
x_4=
\left\{
\begin{array}{ll}
1 & \mbox{if in class D}\\
0 &  \mbox{otherwise}
\end{array}
\right.
$$

### Dealing with missing values
The traing data set contains 19622 records and 160 variables, and 100 out of the 160 variables contain a large number of empty values or NAs. Through examining these variables, we discovered that almost all of them are descriptive statistics, for example, min, max, skewness, kurtosis, average, standard deviation, variance and so on. We decided to remove these variables from our analysis. Also, the index variable, the user names variable, cvtd_timestamp, new_window and num_windown variables are excluded from our analysis. Now the new 
training data set contains 55 variables and 19622 records.

### Splitting the dataset
Since the dataset contains 19622 records, we split the dataset into training, testing and validation sets, with the proportion being approximately 60%, 20% and 20%, respectively. Now the training, testing and validation sets contains 11774,
3924 and 3924 records, respectively. Since in the original dataset the ratio of the observations belonging to the five classes is approximately 0.284, 0.194, 0.174, 0.164, 0.184, we make the following sampling plans:
$$
\begin{array}{|c|c|c||c|}
\hline
& \mbox{training} & \mbox{testing} & \mbox{validation}\\
\hline
\mbox{Class A} & 3344 & 1114 & 1122\\
\mbox{Class B} & 2284 & 761 & 752 \\
\mbox{Class C} & 2049 & 683 & 690 \\
\mbox{Class D} & 1931 & 644 & 641\\
\mbox{Class E} & 2166 & 722 & 719\\
\hline
\hline
\mbox{sum} & 11774 & 3924 & 3924\\
\hline
\end{array}
$$

<!--### Feature selection-->
<!To see whether there exists multicollinearity between the explanatory variables,
we calculated the correlation matrix and from the output below we can see that
there exists moderate to strong correlation between some of the explanatory variables.>

<!```{r,echo=FALSE}
library(knitr)
source
('machine_learning_project_code.R')
```>

<!```{r echo=TRUE}
sum(cor(training.new[,1:30],training.new[,31:54])[,]>0.5)
sum(cor(training.new[,1:30],training.new[,31:54])[,]>0.7)
sum(cor(training.new[,1:30],training.new[,31:54])[,]>0.8)
sum(cor(training.new[,1:30],training.new[,31:54])[,]>0.82)
```>

<!We will conduct the principal component analysis here to remove the multicollinearity.>

##Model building

We selected random forest as our prediction methods. And we used repeated K-fold cross validation. Here $k=10$ and the number of repetition is set to be 3. The output for the random forest is as follows: 

<!```{r, echo=FALSE, cache=FALSE}
library(knitr)
source ('machine_learning_project_code.R')
```>
```{r, echo=FALSE}
library(knitr)
#read in testing and training data
testing<-read.csv('pml-testing.csv')
training<-read.csv('pml-training.csv')
#remove index, user_name, cvtd_timestamp, new_window and num_windown
training.new<-cbind(training[,3:4],training[,8:11],training[,37:49],training[,60:68],
                    training[,84:86],training[,101:102],training[,113:124],
                    training[,139:140],training[,151:160])
training.new<-training.new[,-32]
training.new<-training.new[,-45]
#splitting data
#
training.new.A<-training.new[training.new[,55]=='A',]
training.new.B<-training.new[training.new[,55]=='B',]
training.new.C<-training.new[training.new[,55]=='C',]
training.new.D<-training.new[training.new[,55]=='D',]
training.new.E<-training.new[training.new[,55]=='E',]
dim(training.new.A)
dim(training.new.B)
dim(training.new.C)
dim(training.new.D)
dim(training.new.E)
#
training.new.trs.A.index<-sample(5580, size=3344, replace=F)
training.new.trs.B.index<-sample(3797, size=2284, replace=F)
training.new.trs.C.index<-sample(3422, size=2049, replace=F)
training.new.trs.D.index<-sample(3216, size=1931, replace=F)
training.new.trs.E.index<-sample(3607, size=2166, replace=F)

training.new.A.trs<-training.new.A[training.new.trs.A.index,]
training.new.B.trs<-training.new.B[training.new.trs.B.index,]
training.new.C.trs<-training.new.C[training.new.trs.C.index,]
training.new.D.trs<-training.new.D[training.new.trs.D.index,]
training.new.E.trs<-training.new.E[training.new.trs.E.index,]
training.new.trs<-rbind(training.new.A.trs,training.new.B.trs,training.new.C.trs,
                        training.new.D.trs,training.new.E.trs)
training.new.trs<-data.frame(training.new.trs)
#
training.new.A.inter<-training.new.A[-training.new.trs.A.index,]
training.new.B.inter<-training.new.B[-training.new.trs.B.index,]
training.new.C.inter<-training.new.C[-training.new.trs.C.index,]
training.new.D.inter<-training.new.D[-training.new.trs.D.index,]
training.new.E.inter<-training.new.E[-training.new.trs.E.index,]

testing.new.A.index<-sample(2236, size=1114, replace=F)
testing.new.B.index<-sample(1513, size=761, replace=F)
testing.new.C.index<-sample(1373, size=683, replace=F)
testing.new.D.index<-sample(1285, size=644, replace=F)
testing.new.E.index<-sample(1441, size=722, replace=F)

testing.new.A<-training.new.A.inter[testing.new.A.index,]
testing.new.B<-training.new.B.inter[testing.new.B.index,]
testing.new.C<-training.new.C.inter[testing.new.C.index,]
testing.new.D<-training.new.D.inter[testing.new.D.index,]
testing.new.E<-training.new.E.inter[testing.new.E.index,]
testing.new<-rbind(testing.new.A,testing.new.B,testing.new.C,testing.new.D,testing.new.E)
testing.new<-data.frame(testing.new)

validating.new.A<-training.new.A.inter[-testing.new.A.index,]
validating.new.B<-training.new.B.inter[-testing.new.B.index,]
validating.new.C<-training.new.C.inter[-testing.new.C.index,]
validating.new.D<-training.new.D.inter[-testing.new.D.index,]
validating.new.E<-training.new.E.inter[-testing.new.E.index,]
validating.new<-rbind(validating.new.A,validating.new.B,validating.new.C,validating.new.D,validating.new.E)

validating.new<-data.frame(validating.new)
```


```{r, echo=TRUE, eval=TRUE}
library(caret)
set.seed(1234)
library(randomForest)

ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 3)

model<-train(training.new.trs$classe~.,
              method='rf', data=training.new.trs, preProc=c("center","scale"),
              trControl = ctrl)
model
```

We apply this model to the test dataset 

```{r, echo=TRUE, eval=TRUE}
pred<-predict(model,testing.new)
Predright_prop<-(sum(pred==testing.new[,55])/nrow(testing.new))
table(pred,testing.new[,55])
Predright_prop
```

and to the validation dataset

```{r,echo=TRUE, eval=TRUE}
pred.val<-predict(model,validating.new)
Predright_prop2<-(sum(pred.val==validating.new[,55])/nrow(validating.new))
table(pred.val,validating.new[,55])
Predright_prop2
```
## Prediction
We used the model built to the test dataset given
```{r, echo=FALSE, eval=TRUE}
testing.20<-cbind(testing[,3:4],testing[,8:11],testing[,37:49],testing[,60:68],
                  testing[,84:86],testing[,101:102],testing[,113:124],
                  testing[,139:140],testing[,151:159])
testing.20<-testing.20[,-32]
testing.20<-testing.20[,-45]
```

```{r, echo=TRUE, eval=TRUE}
testing.20<-data.frame(testing.20)
pred.20<-predict(model,testing.20)
pred.20
```


